{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- execute this cell before continue -->\n",
       "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Lato\">\n",
       "<style>.reveal * { font-family: \"Lato\" !important; } .reveal .code_cell * { font-family: monospace !important; }</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<!-- execute this cell before continue -->\n",
    "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Lato\">\n",
    "<style>.reveal * { font-family: \"Lato\" !important; } .reveal .code_cell * { font-family: monospace !important; }</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h1 style=\"color: white; position: absolute; top:30%; left:10%;\">\n",
    "    Web Scraping in Python\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:43%; left:10%;\">\n",
    "    David Mertz, Ph.D.\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98864025-08deda80-2448-11eb-9600-22aa17884cdf.png\" style=\"height: 100%; max-height: inherit; position: absolute; top: 20%; left: 0px;\"></img>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"font-weight: bold;\">\n",
    "    David Mertz\n",
    "</h2>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; margin-top: 0.8em\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "<hr>\n",
    "<br><br>\n",
    "\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    mertz@kdm.training\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    @mertz_david\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    linkedin.com/in/dmertz\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Beautiful Soup\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Beautiful Soup is a versatile library for extracting from and manipulating HTML documents.  Lending its colorful name, Beautiful Soup will also process the *almost-HTML* documents that are common on the World Wide Web, with markup that is not *quite* grammatical HTML (including fragments that could be part of a valid document).  Beautiful Soup will parse \"tag soups\" that are merely largely structured (similar to the \"quirks mode\" of popular web browsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beautiful Soup is **not** a tool for obtaining any documents from the web, but only for processing them once obtained.  For many documents, the `requests` third-party library, or even the standard library `urllib` are sufficient.  For more dynamic website, see the later lessons in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "At the start, let us import a few capabilities, as commonly in these courses.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Getting a feel\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Let us look at a Wikipedia web page.  Like most modern web pages, this page has a great deal of CSS styling, a little JavaScript, many `class` and `id` elements within tags, and quite a bit of nesting of elements, especially within `<span>` and `<div>` elements that have CSS attributes.\n",
    "\n",
    "For a stipulated goal, we would like to pull out all the related pages listed under the \"See also\" section.  Quite likely, this same code will work for other Wikipedia pages, which are similar in markup and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use specific snapshot version for permanence of content\n",
    "url = \"https://en.wikipedia.org/w/index.php?title=Web_scraping&oldid=986505339\"\n",
    "resp = requests.get(url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "Converting a document text to a beautiful soup gives as a nested collection of *nodes*, each of which has numerous useful methods and attributes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Web scraping - Wikipedia</title>\n",
      "Web scraping - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(resp.text)\n",
    "print(soup.title)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "We know (or suspect) that the \"See also\" is one of the `<h2>` elements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contents',\n",
       " 'History[edit]',\n",
       " 'Techniques[edit]',\n",
       " 'Software[edit]',\n",
       " 'Legal issues[edit]',\n",
       " 'Methods to prevent web scraping[edit]',\n",
       " 'See also[edit]',\n",
       " 'References[edit]',\n",
       " 'Navigation menu']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text for e in soup.find_all('h2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "Let us find the element itself using that knowledge.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>\n",
      " <span class=\"mw-headline\" id=\"See_also\">\n",
      "  See also\n",
      " </span>\n",
      " <span class=\"mw-editsection\">\n",
      "  <span class=\"mw-editsection-bracket\">\n",
      "   [\n",
      "  </span>\n",
      "  <a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=17\" title=\"Edit section: See also\">\n",
      "   edit\n",
      "  </a>\n",
      "  <span class=\"mw-editsection-bracket\">\n",
      "   ]\n",
      "  </span>\n",
      " </span>\n",
      "</h2>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "see_also = [e for e in soup.find_all('h2') if e.text.startswith('See also')][0]\n",
    "print(see_also.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "Of course, what we want is the stuff that comes *after* the actual \"See also\" heading.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"div-col columns column-width\" style=\"-moz-column-width: 22em; -webkit-column-width: 22em; column-width: 22em;\">\n",
      " <ul>\n",
      "  <li>\n",
      "   <a href=\"/wiki/Archive.today\" title=\"Archive.today\">\n",
      "    Archive.today\n",
      "   </a>\n",
      "  </li>\n",
      "  <li>\n",
      "   <a href=\"/wiki/Comparison_of_feed_aggregators\" title=\"Comparison of feed aggregators\">\n",
      "    Comparison of feed aggregators\n",
      "   </a>\n",
      "  </li>\n",
      "  <li>\n",
      "   <a href=\"/wiki/Data_scraping\" title=\"Data scraping\">\n",
      "    Data scraping\n",
      "   </a>\n",
      "  </li>\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "see_also_links = see_also.find_next_sibling()\n",
    "print(see_also_links.prettify()[:474], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Finding just the link elements within our current focus:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Archive.today\" title=\"Archive.today\">Archive.today</a>,\n",
       " <a href=\"/wiki/Comparison_of_feed_aggregators\" title=\"Comparison of feed aggregators\">Comparison of feed aggregators</a>,\n",
       " <a href=\"/wiki/Data_scraping\" title=\"Data scraping\">Data scraping</a>,\n",
       " <a href=\"/wiki/Data_wrangling\" title=\"Data wrangling\">Data wrangling</a>,\n",
       " <a href=\"/wiki/Importer_(computing)\" title=\"Importer (computing)\">Importer</a>,\n",
       " <a href=\"/wiki/Job_wrapping\" title=\"Job wrapping\">Job wrapping</a>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see_also_links.find_all('a')[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Perhaps we want just the names of the related concepts:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive.today',\n",
       " 'Comparison of feed aggregators',\n",
       " 'Data scraping',\n",
       " 'Data wrangling',\n",
       " 'Importer',\n",
       " 'Job wrapping',\n",
       " 'Knowledge extraction',\n",
       " 'OpenSocial',\n",
       " 'Scraper site']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.text for e in see_also_links.find_all('a')][:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Or only the link URIs (relative links, in this case, within the same domain):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Archive.today',\n",
       " '/wiki/Comparison_of_feed_aggregators',\n",
       " '/wiki/Data_scraping',\n",
       " '/wiki/Data_wrangling',\n",
       " '/wiki/Importer_(computing)',\n",
       " '/wiki/Job_wrapping',\n",
       " '/wiki/Knowledge_extraction',\n",
       " '/wiki/OpenSocial',\n",
       " '/wiki/Scraper_site']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e['href'] for e in see_also_links.find_all('a')][:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    A simple Wikipedia crawler\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Given what we have seen with only a small part of Beautiful Soup capabilities, we can write a small web scraper to do the following.\n",
    "\n",
    "* Given a simple name that *might* be a Wikipedia page title as a function argument\n",
    "* Massage the name to *probably* match the pattern of URLs\n",
    "* Return None if no such page exists\n",
    "* If a page exists return a dictionary with search term as a key, and the page corresponding to each \"see also\" items likewise contributing keys from it \"see also\".\n",
    "\n",
    "```python\n",
    "{'Foobar': ['Foo', 'Bar', 'Baz'],\n",
    " 'Foo': ['Fliz', 'Flam'],\n",
    " 'Bar': ['Bim', 'Bop', 'Foobar'],\n",
    " 'Baz': ['Do', 'Wap', 'Diddy']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img> \n",
    "\n",
    "A small support function can make Wikipedia URLs from article names\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def urlify(name):\n",
    "    # Make Wikipedia URLs from article titles\n",
    "    if name.startswith(('https://', 'http://')):\n",
    "        return name  # Already a URL\n",
    "    else:\n",
    "        base = 'https://en.wikipedia.org/wiki/'\n",
    "        # Space to underscore in Wikipedia URLs\n",
    "        url = f\"{base}{quote(name.replace(' ', '_'))}\"\n",
    "        return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "The main crawler:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def see_also_graph(name, _depth=0, maxdepth=1):\n",
    "    graph = dict()\n",
    "    resp = requests.get(urlify(name))\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Unable to find page at\", urlify(name))\n",
    "        return None\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    see_also = [e for e in soup.find_all('h2') \n",
    "                  if e.text.startswith(\"See also\")]\n",
    "    if see_also:   # Some outgoing links\n",
    "        see_also_links = see_also[0].find_next_sibling('ul')\n",
    "        names = [e.text for e in see_also_links.find_all('a')]\n",
    "        graph[name] = names\n",
    "        if _depth < maxdepth:   # Limited recursion depth\n",
    "            for name in names:\n",
    "                graph.update(see_also_graph(name, _depth=_depth+1))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Crawling from a particular starting article.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Percent-encoding': ['Internationalized Resource Identifier',\n",
       "  'Punycode',\n",
       "  'Binary-to-text encoding',\n",
       "  'Shellcode'],\n",
       " 'Internationalized Resource Identifier': ['IDN',\n",
       "  'Semantic Web',\n",
       "  'Punycode',\n",
       "  'XRI'],\n",
       " 'Punycode': ['Emoji domain', 'UTF-5', 'UTF-6', 'Website spoofing'],\n",
       " 'Shellcode': ['Alphanumeric code',\n",
       "  'Computer security',\n",
       "  'Buffer overflow',\n",
       "  'Exploit (computer security)',\n",
       "  'Heap overflow',\n",
       "  'Metasploit Project',\n",
       "  'Shell (computing)',\n",
       "  'Shell shoveling',\n",
       "  'Stack buffer overflow',\n",
       "  'Vulnerability (computing)']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see_also_graph('Percent-encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "An article topic might not exist.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find page at https://en.wikipedia.org/wiki/Silly_Page\n"
     ]
    }
   ],
   "source": [
    "see_also_graph('Silly Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Special features\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "There are some edge-case features we have already subtly used that it is worth noting.  Although the Wikipdia pages looked at are complete HTML pages, Beautiful Soup will also parse fragments of HTML (or near-HTML).\n",
    "\n",
    "Beautiful Soup supports multiple parsers, all of which are able to deal with not-quite-well-formed HTML.  The default parser is `html.parser` that only relies on the Python standard library.  If installed, the external libraries `lxml` or `html5lib` may be used.  I recommend lxml if it is an option (the library is discussed in the Python Serialization INE course as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "The different libraries will parse ill-formed HTML slightly differently.  The differences will rarely matter to most scripts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><a></a></body></html>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p></a>\", \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><a><p></p></a></body></html>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p></a>\", \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a></a>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(\"<a></p></a>\", \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Text versus string\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "In the above examples, I sometimes used the attribute `.string` on nodes, and other times `.text`.  These two attributes are similar, but also slightly different.  Use `.text` to pull out all the plain text from inside a node, even if other tags are nested.  In contrast, `.string` will only pull out the element body if it is *only* text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also look at an example of using a child element tag name (`<p>` in this case) as an attribute of a soup to access it.  Within the fragment.  The attribute style will simply get the first child.  To find other children or siblings, use attributes like `.next_sibling` and `.previous_sibling`, or `.next_siblings` and `.previous_siblings` for multiple, or methods like `.find()` and `.find_all()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Parse a fragment of HTML.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>This text has <i>italics</i> and <b>boldface</b></p>\n",
      "None\n",
      "This text has italics and boldface\n",
      "['\\n', <p>Another paragraph</p>, '\\n']\n"
     ]
    }
   ],
   "source": [
    "fragment1 = \"\"\"<div>\n",
    "<h1>Header</h1>\n",
    "<p>This text has <i>italics</i> and <b>boldface</b></p>\n",
    "<p>Another paragraph</p>\n",
    "</div>\"\"\"\n",
    "\n",
    "soup1 = BeautifulSoup(fragment1)\n",
    "print(soup1.p)\n",
    "print(soup1.p.string)\n",
    "print(soup1.p.text)\n",
    "print(list(soup1.p.next_siblings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When there is, indeed, only body text (PCDATA, in XML terms), the two attributes still give you slightly different objects back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><div>\n",
      "<h1>Header</h1>\n",
      "<p>This text has only plain text in paragraph</p>\n",
      "<p>Another paragraph</p>\n",
      "</div></body></html> \n",
      "\n",
      "This text has only plain text in paragraph <class 'bs4.element.NavigableString'>\n",
      "This text has only plain text in paragraph <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "fragment2 = \"\"\"<div>\n",
    "<h1>Header</h1>\n",
    "<p>This text has only plain text in paragraph</p>\n",
    "<p>Another paragraph</p>\n",
    "</div>\"\"\"\n",
    "\n",
    "soup2 = BeautifulSoup(fragment2)\n",
    "print(soup2, '\\n')\n",
    "print(soup2.p.string, type(soup2.p.string))\n",
    "print(soup2.p.text, type(soup2.p.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `NavigableString` that we got from a `.string` attribute will work as a string in other functions.  It is a subclass of string, so you can pass it to your regular text manipulation functions.  But it also adds some characteristic soup methods and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Text Has Only Plain Text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<p>Another paragraph</p>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def title(s, nwords=6):\n",
    "    words = s.split()[:nwords]\n",
    "    s = ' '.join(words)\n",
    "    return s.title()\n",
    "\n",
    "para_text = soup2.p.string\n",
    "print(title(para_text))\n",
    "# Navigating from the string, not from the tag\n",
    "para_text.find_next('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Character encoding\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "In the best of circumstances, web pages will always be encoded as UTF-8.  Sometimes it doesn't turn out that well.  Beautiful Soup will use *chardet*-style heuristics to try to guess encodings.  This usually works, but you can manually specify if it does not.  Of course, that only helps if you *know* the correct encoding.  \n",
    "\n",
    "Often Beautiful Soup will even detect \"mixed encodings\" where special characters from Windows-1252 (usually \"smart quotes\") are illegally interspersed with Unicode.  The library really is clever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<div style=\"position: relative; text-align: left;\">\n",
    "There are times when a META tag within an HTML document will indicate a character encoding, e.g.\n",
    "\n",
    "```html\n",
    "<meta content=\"text/html; charset=ISO-Latin-1\" http-equiv=\"Content-type\" />\n",
    "```\n",
    "\n",
    "It is **possible** that this tag is telling the truth.  Beautiful Soup is smart enough to look at the advice, but also not fail it it is inaccurate.  For example, here is a document that contains an illegal character for its declaration.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<body><p>Sacré bleu!</p></body>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = b'''<html>\n",
    "  <head><meta content=\"text/html; charset=utf-8\"/></head>\n",
    "  <body><p>Sacr\\xe9 bleu!</p></body>\n",
    "</html>'''\n",
    "\n",
    "soup = BeautifulSoup(page)\n",
    "print(soup.original_encoding)\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso-8859-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<body><p>Greek Åëëçíéêü</p></body>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = b'''\n",
    " <html>\n",
    "  <head><meta content=\"text/html; charset=\"iso-8859-1\"/></head>\n",
    "  <body><p>Greek \\xc5\\xeb\\xeb\\xe7\\xed\\xe9\\xea\\xfc</p></body>\n",
    " </html>\n",
    "'''\n",
    "soup = BeautifulSoup(page)\n",
    "print(soup.original_encoding)\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-8859-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<body><p>Greek Ελληνικό</p></body>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = b'''\n",
    " <html>\n",
    "  <head><meta content=\"text/html; charset=\"iso-8859-1\" /></head>\n",
    "  <body><p>Greek \\xc5\\xeb\\xeb\\xe7\\xed\\xe9\\xea\\xfc</p></body>\n",
    " </html>\n",
    "'''\n",
    "soup = BeautifulSoup(page, exclude_encodings=[\"iso-8859-1\", \"windows-1252\"])\n",
    "print(soup.original_encoding)\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso-8859-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<body><p>Greek Ελληνικό</p></body>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = b'''\n",
    " <html>\n",
    "  <body><p>Greek \\xc5\\xeb\\xeb\\xe7\\xed\\xe9\\xea\\xfc</p></body>\n",
    " </html>\n",
    "'''\n",
    "soup = BeautifulSoup(page, from_encoding=\"iso-8859-7\")\n",
    "print(soup.original_encoding)\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Searching for content\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "We have already looked at several methods that will search for tags.  In general, the variations including \"next\" will look at the same level of the hierarchy, while the variations including \"find\" will search across levels.  All of these share almost all the same potential arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "The above examples only looked particular tag names, but other options are available.  Let us look through a book available from Project Gutenberg (PG).  Note that repeated web crawling of PG is not permitted; questions and concerns about this are discussed in lesson 4 of this course.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/25007/25007-h/25007-h.htm\"\n",
    "resp = requests.get(url)\n",
    "soups = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "      The Project Gutenberg eBook of Fifty Soups, by Thomas J. Murrey\r\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(soups)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beyond searching for a certain tag name, we might also search for tags belonging to a certain class. E.g. tags that contain `class=\"smcap\"`.  In this book, and within PG, this indicates a table-of-contents listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"smcap\">Copyright, 1884</span>,\n",
       " <span class=\"smcap\">By WHITE, STOKES, &amp; ALLEN.</span>,\n",
       " <span class=\"smcap\">Remarks on Soups</span>,\n",
       " <span class=\"smcap\">Artichoke Soup</span>,\n",
       " <span class=\"smcap\">Asparagus Soup</span>,\n",
       " <span class=\"smcap\">Barley Soup</span>,\n",
       " <span class=\"smcap\">Beans, Puree of</span>,\n",
       " <span class=\"smcap\">Beef Stock</span>,\n",
       " <span class=\"smcap\">Beef Tea</span>,\n",
       " <span class=\"smcap\">Bouille-Abaisse</span>,\n",
       " <span class=\"smcap\">Cauliflower Soup</span>,\n",
       " <span class=\"smcap\">Celery, Cream of</span>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc = soup.find_all(class_=\"smcap\")  # `class` is reserved, use trailing underscore\n",
    "toc[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "Most attribute names can simply be specified as named parameters to a search method.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a id=\"Page_7\" name=\"Page_7\">7</a>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id=\"Page_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "The word `name` is reserved for the tag name, but you can specify the attribute `name` with a slight circumlocution (an HTML attribute `attrs` would require this same style, if it occurred in the document).  This is also necessary when attributes (or tag names) contain characters outside those allowed in Python identifiers (most often dashes).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a id=\"Page_7\" name=\"Page_7\">7</a>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(attrs={\"name\": \"Page_7\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "You might search for text content *within* an element.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"smcap\">Artichoke Soup</span>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.parent for e in soup.find_all(string=\"Artichoke Soup\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A problem here is that a string search only matches *exactly* that element body.  If you want more flexibility, you can use a regular expression pattern to search for substrings or more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"smcap\">Artichoke Soup</span>,\n",
       " <b>Artichoke Soup.</b>,\n",
       " <p><b>Artichoke Soup.</b>—Melt a piece of butter the size of an egg in a\n",
       " saucepan; then fry in it one white turnip sliced, one red onion sliced,\n",
       " three pounds of Jerusalem artichokes washed, pared, and sliced, and a\n",
       " rasher of bacon. Stir these in the boiling butter for about ten minutes,\n",
       " add gradually one pint of stock. Let all boil together until the\n",
       " vegetables are thoroughly cooked, then add three pints more of stock;\n",
       " stir it well; add pepper and salt to taste, strain and press the\n",
       " vegetables through a sieve, and add one pint of boiling milk. Boil for\n",
       " five minutes more and serve.</p>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.parent for e in soup.find_all(string=re.compile('[Aa]rtichoke'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "Most sophisticated of all the options is to write a custom function to find the elements of relevance to your purpose.  To contrive an example, this PG book has markers indicating where the page breaks occurred in the original text.  \n",
    "\n",
    "Let us identify those paragraphs that cross a page break and contain eggs in their recipe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def egg_near_pagebreak(e):\n",
    "    if e.name == 'p':\n",
    "        if e.find(class_=\"pagenum\"):\n",
    "            if 'egg' in e.text:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "Using our custom search function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9: Milk or cream should be boiled and strained9 and added hot when intended\r\n",
      "for soups; when eggs are used beat them thoroughly, and  ...\n",
      "\n",
      "Page 10: When the vegetables are thoroughly cooked, strain the soup into a large\r\n",
      "saucepan, and set10 it on back of range to keep hot, but  ...\n",
      "\n",
      "Page 15: Bisque of Lobster.—Procure two large live lobsters; chop them up while\r\n",
      "raw, shells and all; put them into a mortar with three-fou ...\n",
      "\n",
      "Page 26: The yolks of the eggs may be worked to a26 paste, and made into round\r\n",
      "balls to imitate turtle eggs if this is desired. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for recipe in soup.find_all(egg_near_pagebreak):\n",
    "    page = recipe.find('a')\n",
    "    print(f\"Page {page.text}: {recipe.text[:130]} ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Summary\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "\n",
    "In this lesson we have seen a good subset of the Beautiful Soup API.  The documentation will detail additional methods on node objects, but working with them is largely similar to those we have seen.\n",
    "\n",
    "As with XML libraries like `xml.dom` or ElementTree, Beautiful Soup also has methods to add or rearrange elements and write out new HTML documents.  The `.prettyprint()` method we saw mostly suffices for the latter.  Those capabilities are useful, but not central to the purpose of web *scraping*."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
