{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- execute this cell before continue -->\n",
       "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Lato\">\n",
       "<style>.reveal * { font-family: \"Lato\" !important; } .reveal .code_cell * { font-family: monospace !important; }</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<!-- execute this cell before continue -->\n",
    "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Lato\">\n",
    "<style>.reveal * { font-family: \"Lato\" !important; } .reveal .code_cell * { font-family: monospace !important; }</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h1 style=\"color: white; position: absolute; top:30%; left:10%;\">\n",
    "    Web Scraping in Python\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:43%; left:10%;\">\n",
    "    David Mertz, Ph.D.\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98864025-08deda80-2448-11eb-9600-22aa17884cdf.png\" style=\"height: 100%; max-height: inherit; position: absolute; top: 20%; left: 0px;\"></img>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"font-weight: bold;\">\n",
    "    David Mertz\n",
    "</h2>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; margin-top: 0.8em\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "<hr>\n",
    "<br><br>\n",
    "\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    mertz@kdm.training\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    @mertz_david\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    linkedin.com/in/dmertz\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Scrapy\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Unlike Beautiful Soup which is a library for parsing and manipulating HTML files, Scrapy is a full-fledged *spider* (a.k.a. *web crawler*, or *robot*) that focuses on retrieving and processing many web pages during a run.  Generally, `scrapy` is used as a command line tool that will utilize scripts, and provides various command-line switches for that use. This can also easily be put into a cronjob or other automated scheduling.\n",
    "\n",
    "Scrapy utilizes XPATH and CSS selectors to identify portions of a web page scraped.  Optionally, you are perfectly free to perform the actual parsing portion using the now-familiar Beautiful Soup library.  Moreover, Scrapy includes asynchronous scheduling of requests for many web pages.  Unlike the sequential scraper we wrote in the prior lesson, scripts for Scrapy will concurrently download the resources identified (and perhaps queued as a result of finding links within scraped pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "\n",
    "At the start, let us import a few capabilities, as commonly in these courses.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    A basic preview\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "The first example shown here is based on an early one from Scrapy's documentation.  In particular, it scrapes the test domain `toscrape.com` that is provided for just this kind of practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First we can look at the source code of the file defining the scraper, then we can run it from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'Quotes'\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "```\n",
    "```python\n",
    "    custom_settings = {\n",
    "        \"DEPTH_LIMIT\": 1,\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 3,\n",
    "        'ROBOTSTXT_OBEY': True,\n",
    "        'DOWNLOAD_DELAY': 0.25,\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            text = quote.css('span.text::text').get()\n",
    "            yield {'author': quote.xpath('span/small/text()').get(),\n",
    "                   'text': text.replace('“', '').replace('”', '')}\n",
    "  \n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "The output to STDERR from the run of the spider is fairly long. Let us save it in a file to look through.  Here we run the `QuoteScrape.py` file shown and send output as JSON Lines to `quotes.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -f quotes.jl\n",
    "!scrapy runspider QuoteScrape.py -o quotes.jl -L INFO 2>quotescrape.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-16 15:34:50 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\r\n",
      "2020-11-16 15:34:50 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 08:20:52) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-5.4.0-7634-generic-x86_64-with-glibc2.10\r\n",
      "2020-11-16 15:34:50 [scrapy.crawler] INFO: Overridden settings:\r\n",
      "{'CONCURRENT_REQUESTS_PER_DOMAIN': 3,\r\n",
      " 'DEPTH_LIMIT': 1,\r\n",
      " 'DOWNLOAD_DELAY': 0.25,\r\n",
      " 'LOG_LEVEL': 'INFO',\r\n",
      " 'ROBOTSTXT_OBEY': True,\r\n",
      " 'SPIDER_LOADER_WARN_ONLY': True}\r\n",
      "2020-11-16 15:34:50 [scrapy.extensions.telnet] INFO: Telnet Password: a86c7570c2ef38c3\r\n",
      "2020-11-16 15:34:50 [scrapy.middleware] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.corestats.CoreStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\r\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\r\n",
      " 'scrapy.extensions.logstats.LogStats']\r\n"
     ]
    }
   ],
   "source": [
    "!sed -n '1,16p' quotescrape.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-16 15:34:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n"
     ]
    }
   ],
   "source": [
    "!sed -n '17,29p' quotescrape.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-16 15:34:50 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2020-11-16 15:34:50 [scrapy.middleware] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2020-11-16 15:34:50 [scrapy.core.engine] INFO: Spider opened\r\n",
      "2020-11-16 15:34:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2020-11-16 15:34:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\r\n",
      "2020-11-16 15:34:51 [scrapy.core.engine] INFO: Closing spider (finished)\r\n",
      "2020-11-16 15:34:51 [scrapy.extensions.feedexport] INFO: Stored jl feed (20 items) in: quotes.jl\r\n"
     ]
    }
   ],
   "source": [
    "!sed -n '30,42p' quotescrape.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-16 15:34:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/request_bytes': 724,\r\n",
      " 'downloader/request_count': 3,\r\n",
      " 'downloader/request_method_count/GET': 3,\r\n",
      " 'downloader/response_bytes': 5642,\r\n",
      " 'downloader/response_count': 3,\r\n",
      " 'downloader/response_status_count/200': 2,\r\n",
      " 'downloader/response_status_count/404': 1,\r\n",
      " 'elapsed_time_seconds': 0.929831,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2020, 11, 16, 20, 34, 51, 489919),\r\n",
      " 'item_scraped_count': 20,\r\n",
      " 'log_count/INFO': 11,\r\n",
      " 'memusage/max': 56225792,\r\n",
      " 'memusage/startup': 56225792,\r\n",
      " 'request_depth_max': 1,\r\n",
      " 'response_received_count': 3,\r\n",
      " 'robotstxt/request_count': 1,\r\n",
      " 'robotstxt/response_count': 1,\r\n",
      " 'robotstxt/response_status_count/404': 1,\r\n",
      " 'scheduler/dequeued': 2,\r\n",
      " 'scheduler/dequeued/memory': 2,\r\n",
      " 'scheduler/enqueued': 2,\r\n",
      " 'scheduler/enqueued/memory': 2,\r\n",
      " 'start_time': datetime.datetime(2020, 11, 16, 20, 34, 50, 560088)}\r\n",
      "2020-11-16 15:34:51 [scrapy.core.engine] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "!sed -n '43,99p' quotescrape.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Let us take a quick look to find out what we scraped.  In particular, there are 10 quotes per page at the URL, so limiting the depth of crawling to 1 should only fetch the first 20.  JSON Lines in a local file is a convenient format to use here, but we could configure it to store over FTP, AWS S3, Google Cloud Storage, and in formats like XML, Python pickle, or CSV (or even plugin custom backends like an RDBMS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 quotes scraped\n",
      "Example: André Gide\n",
      "It is better to be hated for what you are than to be loved for what you are not.\n"
     ]
    }
   ],
   "source": [
    "with open('quotes.jl') as fh:\n",
    "    quotes = [json.loads(quote) for quote in fh]\n",
    "\n",
    "print(f\"{len(quotes)} quotes scraped\")\n",
    "print(f\"Example: {quotes[6]['author']}\")\n",
    "print(quotes[6]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Parsing with Beautiful Soup\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "If we wish to use Beautiful Soup to identify portions of the document we scrape, instead of the native XPATH and CSS selectors that Scrapy uses, we can easily substitute that portion.  Whether a given query is easier to express as a soup or as XPATH/CSS will vary according to task and person preference; Beautiful Soup is more robust against ill-formed HTML though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'Quotes'\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "    \n",
    "    custom_settings = {\n",
    "        \"DEPTH_LIMIT\": 1,\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        for quote in soup.find_all('div', class_='quote'):\n",
    "            text = quote.find('span', class_='text').string\n",
    "            yield {'author': text.find_next(class_=\"author\").text,\n",
    "                   'text': text.replace('“', '').replace('”', '')}\n",
    "  \n",
    "        next_page = soup.find('li', class_='next').find('a')['href']\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "If we increase the logging level in the `scrapy` command line, there will normally be no output to worry about redirecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -f quotes2.jl\n",
    "!scrapy runspider QuoteScrapeBS.py -o quotes2.jl -L ERROR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 quotes scraped\n",
      "Example: Steve Martin\n",
      "A day without sunshine is like, you know, night.\n"
     ]
    }
   ],
   "source": [
    "with open('quotes2.jl') as fh:\n",
    "    quotes = [json.loads(quote) for quote in fh]\n",
    "\n",
    "print(f\"{len(quotes)} quotes scraped\")\n",
    "print(f\"Example: {quotes[9]['author']}\")\n",
    "print(quotes[9]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Logging in and credentials\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Where Scrapy provides a strong advantage of simply using `requests` and Beautiful Soup is where a certain degree of authomation is needed.  For example, logging into a web site before performing scraping actions is commonly needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is common for web sites include `<input type=\"hidden\">` elements within login forms.  These will carry session data or authentication tokens that are important to preserve.  Using `scrapy.FormRequest.from_response)` captures all of those and can override only the form fields needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "As an example, we will login to the same `toscrape.com` domain in earlier examples.  It provides a \"Login\" link that must be followed to get to the form.  Any credentials succeed in this case, but the code simulates a failure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "from random import random\n",
    "\n",
    "def authentication_failed(response):\n",
    "    # Check the contents of the response, True if failed\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    has_logout = [a for a in soup.find_all('a') if a.text == 'Logout']\n",
    "    # randomly fail sometimes\n",
    "    return not has_logout or random() > 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "class LoginSpider(scrapy.Spider):\n",
    "    name = 'Login quotes.toscrape.com'\n",
    "    start_urls = [\"http://quotes.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        login_link = [a['href'] for a in soup.find_all('a') \n",
    "                                if a.text == 'Login'][0]\n",
    "        return response.follow(login_link, self.login)\n",
    "\n",
    "    def login(self, response):\n",
    "        self.user = choice(['user1', 'user2', 'user3', 'user4'])\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response, callback=self.after_login, \n",
    "            formdata={'username': self.user, 'password': 'pw'})\n",
    "\n",
    "    def after_login(self, response):\n",
    "        if authentication_failed(response):\n",
    "            self.logger.error(f\"Login failed for {self.user}\")\n",
    "            return \n",
    "        # Get one random quote author\n",
    "        return response.follow('/random', self.author)\n",
    "          \n",
    "    def author(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {'author': quote.xpath('span/small/text()').get()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>   \n",
    "\n",
    "We can run the scraper a few times to encounter the failure and success of login on a given attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"author\": \"Mark Twain\"}\r\n"
     ]
    }
   ],
   "source": [
    "!rm -f author.jl\n",
    "!scrapy runspider -o author.jl Login.py -L WARNING\n",
    "!cat author.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Link extraction\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "It would not be so difficult to locate all the links on a page using XPATH or Beautful Soup, but it is a common enough need that Scrapy automates that in a class.  Here is a small program that will find all the new titles published by Project Gutenberg. Use this moderately, but since we do not actually crawl here, the load will not be high on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class PG_NewTitles(scrapy.Spider):\n",
    "    # A snapshot of the current \"new titles\" on Project Gutenberg\n",
    "    name = 'New Titles'\n",
    "    link_extractor = LinkExtractor()\n",
    "    start_urls = ['https://dev.gutenberg.org/browse/recent/last1.html.utf8']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for link in self.link_extractor.extract_links(response):\n",
    "            # Many links to general navigation, a heuristic to narrow results\n",
    "            if 'ebooks' in link.url:\n",
    "                yield {\"title\": link.text, \"url\": link.url}\n",
    "                # This would recurse into linked pages. Not permitted by PG\n",
    "                # yield Request(link.url, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!rm -f newtitles.jl\n",
    "!scrapy runspider -o newtitles.jl New_at_PG.py -L WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Christian serving his own generationA Sermon occasioned by the lamented death of Joseph John Gurney, Esq.\n",
      "https://dev.gutenberg.org/ebooks/63770\n",
      "\n",
      "Christ Remembered at His Table\n",
      "https://dev.gutenberg.org/ebooks/63769\n",
      "\n",
      "Eight Dramas of Calderon\n",
      "https://dev.gutenberg.org/ebooks/63776\n",
      "\n",
      "Poetry of the Anti-jacobinComprising the Celebrated Political and Satirical Poems,of the Rt. Hons. G. Canning, John Hookham Frere, W. Pitt,the Marquis Wellesley, G. Ellis, W. Gifford, the Earl ofCarlisle, and Others.\n",
      "https://dev.gutenberg.org/ebooks/63778\n",
      "\n",
      "Legend\n",
      "https://dev.gutenberg.org/ebooks/63775\n",
      "\n",
      "Amica America\n",
      "https://dev.gutenberg.org/ebooks/63777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in open('newtitles.jl'):\n",
    "    book = json.loads(line)\n",
    "    if book['title'].startswith(('Search and', 'Bookshelves', 'Offline')):\n",
    "        continue\n",
    "    print(f\"{book['title']}\")\n",
    "    print(f\"{book['url']}\")\n",
    "    print()\n",
    "    if (i := i+1) > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Other Scapy tools\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "When you start to use Scrapy as a large scale web scraper, a variety of tools are available to monitor or analyze the behavior.  For example, we already saw the verbose log files that can be produced (much more, in fact, if you use the `DEBUG` level).\n",
    "\n",
    "A few other tools are worth mentioning, but will not be detailed here.  For our examples that limited themselves to scraping only a few pages, these are not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>   \n",
    "</div>\n",
    "\n",
    "**Telnet access**.  A long running spider creates a telnet server that can interrogate features of its run.  Log entries show details about the server, for example:\n",
    "\n",
    "```\n",
    "2020-11-15 16:44:03 [scrapy.extensions.telnet] INFO: Telnet Password: 1fe8a0b3190dad26\n",
    "2020-11-15 16:44:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "```\n",
    "\n",
    "The password will vary each time (and a non-default port will be chosen if one is occupied).  The settings  TELNETCONSOLE_USERNAME and TELNETCONSOLE_PASSWORD may be defined within the spider (default user is \"scrapy\"). The telenet console looks like a regular Python shell, but with some special objects available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "% telnet localhost 6023\n",
    "Trying localhost...\n",
    "Connected to localhost.\n",
    "Escape character is '^]'.\n",
    "Username: scrapy\n",
    "Password: <generated_password>\n",
    "\n",
    ">>> engine.pause()    # pause operation\n",
    ">>> engine.unpause()  # resume operation\n",
    ">>> engine.stop()\n",
    "Connection closed by foreign host.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Of special interest is the `est()` command provided.  E.g.:\n",
    "\n",
    "```python\n",
    ">>> est()\n",
    "Execution engine status\n",
    "\n",
    "time()-engine.start_time                        : 8.62972998619\n",
    "engine.has_capacity()                           : False\n",
    "len(engine.downloader.active)                   : 16\n",
    "engine.scraper.is_idle()                        : False\n",
    "engine.spider.name                              : followall\n",
    "engine.spider_is_idle(engine.spider)            : False\n",
    "engine.slot.closing                             : False\n",
    "len(engine.slot.inprogress)                     : 16\n",
    "len(engine.slot.scheduler.dqs or [])            : 0\n",
    "len(engine.slot.scheduler.mqs)                  : 92\n",
    "len(engine.scraper.slot.queue)                  : 0\n",
    "len(engine.scraper.slot.active)                 : 0\n",
    "engine.scraper.slot.active_size                 : 0\n",
    "engine.scraper.slot.itemproc_size               : 0\n",
    "engine.scraper.slot.needs_backout()             : False\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>  \n",
    "\n",
    "**Scrapy shell**. Using the shell effectively launches a Scrapy engine with no requests pending.  You can interact with this using similar commands as with the telnet shell, or indeed execute arbitrary Python code to explore Scrapy and websites.\n",
    "\n",
    "Let us take a look at such an interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "[webscraping]% scrapy shell 'https://kdm.training' --nolog\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f2fd738b040>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://kdm.training>\n",
    "[s]   response   <200 https://www.gnosis.cx/kdm/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f2fd7388df0>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7f2fd6bf9040>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```python\n",
    "In [1]: crypt = response.xpath('//*[@class=\"__cf_email__\"]/@data-cfemail').get()\n",
    "\n",
    "In [2]: crypt\n",
    "Out[2]: 'f39a9d959cb398979edd8781929a9d9a9d94'\n",
    "    \n",
    "In [3]: def decode(email):\n",
    "   ...:     # Cloudflare obscures email addresses to make harvesting harder\n",
    "   ...:     plaintext = \"\"\n",
    "   ...:     k = int(email[:2], 16)\n",
    "   ...:     for i in range(2, len(email)-1, 2):\n",
    "   ...:         plaintext += chr(int(email[i:i+2], 16)^k)\n",
    "   ...:     return plaintext\n",
    "   ...:\n",
    "\n",
    "In [4]: decode(crypt)\n",
    "Out[4]: 'info@kdm.training'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Summary\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "We have seen both the query languages (XPATH and CSS) that Scrapy provides.  More essential than that, we looked at Scrapy as an *engine* for powering high-performance web crawlers.  Scrapy is able to address form completion, password management, state and cookie handling, and a variety of other needs of robust robots.\n",
    "\n",
    "In the next lesson, we look at the Selenium framework.  It overlaps greatly in capabilities with Scrapy, but utilizes actual web browsers to automate scraping."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
