{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- execute this cell before continue -->\n",
    "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Lato\">\n",
    "<style>.reveal * { font-family: \"Lato\" !important; } .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 { font-family: \"Lato\" !important; } .reveal .code_cell *, .reveal code, .reveal code * { font-family: monospace !important; }</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h1 style=\"color: white; position: absolute; top:30%; left:10%;\">\n",
    "    Web Scraping in Python\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:43%; left:10%;\">\n",
    "    David Mertz, Ph.D.\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98864025-08deda80-2448-11eb-9600-22aa17884cdf.png\" style=\"height: 100%; max-height: inherit; position: absolute; top: 20%; left: 0px;\"></img>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"font-weight: bold;\">\n",
    "    David Mertz\n",
    "</h2>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; margin-top: 0.8em\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "<hr>\n",
    "<br><br>\n",
    "\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    mertz@kdm.training\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    @mertz_david\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    linkedin.com/in/dmertz\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Law, Ethics, and Robots\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "When scraping websites, you should carefully consider the possibility that copyright, publicity rights, or database rights might apply to the content you scraped.  The specific limitations that apply vary greatly by jurisdiction, and INE cannot provide legal advice on these matters.  Nonetheless, in a general sense, the right you have to read a web page for your own personal use may not generalize to a right to republish content on that page, nor even to publish or utilize aggregations or summarizations (such as statistics about or extractions of numeric values) of the content of sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Copyright rules are similar across the world, since nearly all nations are signatories of the Berne Convention.  Publicity rights are more strongly governed in the European Union than in most places.  So-called database rights are excluded in Australia, and in the United States by the 1991 Feist Publications v. Rural Telephone Service Supreme Court case.  But this area becomes increasingly complicated when web servers and scraping robots are located across varying jurisdictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Over and above the rules created by laws and treaties, the IETF draft standard for the Robots Exclusion Standard creates a reasonable ethical standard for uses of websites that should not be automated, or for limitations that automatic access should follow.  For example, a web server may indicate that certain URLs should not be indexed, or that web crawlers should not put an undue burden on a site by too frequent access.  The requests may apply either to specific robots or to all automated programs, in a formally specified manner.  Whether your particular web scraping program constitutes a robot or web crawler under the website's intentions is an additional judgement you need to make; it will be driven by the specifics of your program and of the site.\n",
    "\n",
    "In this lesson we discuss specific technical means to process the robots.txt file and the other exclusion mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Three robot languages\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "There are actually three separate mechanism by which websites might inform users of their intention about use of the site by automated processes.  On top of those three technical mechanisms, if a particular site requires registration to interact with its content, it probably publishes terms-of-service that impose some restrictions.\n",
    "\n",
    "In evaluating what do to with your robots, think about the relationship between your purpose and the guidance the site provides.  As a general rule, creating inadvertent *denial of service attacks* on websites is very bad manners.  It is unfortunately easy to let a particular process run without appropriate limits through small programming errors, hence possibly putting undue load on a server.  Even sites that are happy to be spidered or crawled wish to do so on a reasonable schedule (many will explicitly block you if they detect bad behavior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "The three places to look for site guidance in crawling or scraping a site are:\n",
    "\n",
    "* Link nofollow directive\n",
    "* META robots tag\n",
    "* Robots.txt file\n",
    "\n",
    "The last of those is the most widely used and most often honored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Link nofollow\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Within HTML links—i.e. `<a>` tags—as well as an `href` attribute that specifies where the link points, you commonly encounter a `rel` attribute that describes how the link is related to the current page.  The *relationship* described may have a single value, or it may contain multiple values separated by spaces or commas.\n",
    "\n",
    "Common relationship semantics that may affect your web crawling purposes, but that do not reflect any specific request by the creator of the site, include `author`, `bookmark`, `external`, `help`, `license`, `login`, `logout`, `next`, `prev`, and `search`.  These words are not enforced, and particular websites might use entirely different words.  However, the words are generally meaningful and descriptive, and may help your robot navigate to relevant linked pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Two link relationship terms you might see as `noreferrer` and `noopener`.  These are unlikely to affect your web crawler, but tell a web browser not to tell a destination site where an incoming link came from.\n",
    "\n",
    "Of relevant in designing your web crawler are `nofollow`, `ugc`, and `sponsored`.  The last two are largely Google conventions for \"user-generated content\" and \"sponsored (advertising) link.\"  Google being what it is, many sites decide to add this information.  \n",
    "\n",
    "The key one for you to pay attention to is `nofollow`, which is a *request* by the web site creator for a robot (spider/web crawler) not to follow a given link automatically.  Of course, having that tag on one specific link does not mean that the same URL might not occur elsewhere without that tagged word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Let us read a page I created for examples.  Beautiful Soup will return the various `rel` values as a list if they are space separated, but does not do so automatically for commas. Spaces are the more common convention by site creators, but not universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum None\n",
      "Advertising ['sponsored']\n",
      "User-generated content ['ugc']\n",
      "Nofollow links ['nofollow']\n",
      "Mulitplicity in philosophy ['ugc', 'external', 'author']\n",
      "Input device sharing ['external', 'nofollow', 'sponsored']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://kdm.training/link-relations.html\"\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page)\n",
    "for a in soup.find_all('a'):\n",
    "    rel = a.get('rel')\n",
    "    if rel and len(rel) == 1:  # Split if single comma-separated\n",
    "        rel = rel[0].split(',')\n",
    "    print(a.text, rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "It your decision how to treat these link relationships.  It is easy to forget to check for the `rel` attribute of the `a` tag at all, and most working code probably does so.  Very often that attribute does not exist, in any case. \n",
    "\n",
    "Assuming you are trying to use the guidance, you might write code similar to the following.  The below is simple Beautiful Soup code, but the same concept would apply with Scrapy, or Selenium, or any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def links_to_follow(url, exclude={\"nofollow\", \"sponsored\"}):\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    urls = []\n",
    "    for a in soup.find_all('a'):\n",
    "        rel = a.get('rel') or []\n",
    "        rel = rel[0].split(',') if rel and len(rel) == 1 else rel\n",
    "        if exclude & set(rel):\n",
    "            # Do not spider these type of relations\n",
    "            continue\n",
    "        urls.append(a['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Lorem_ipsum',\n",
       " 'https://en.wikipedia.org/wiki/User-generated_content',\n",
       " 'https://en.wikipedia.org/wiki/Multiplicity_(philosophy)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_to_follow(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    META robots\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Within the header of a web page, one or more `<meta>` tags might occur.  These can serve many purposes, but one is to instruct robots on good behavior.  Each `<meta>` tag is distinguished by having a different `name` attribute indicating its purpose.\n",
    "\n",
    "Historically, the HTML `<meta>` tag was spelled as `<META>` instead.  Usually in such older web pages, the attributes and values are likewise in uppercase.  While that uppercase convention is fairly old, it remains fairly common in published web pages.  Beautiful Soup is aware of this issue, and treats the `<META>` tag as if it were lowercase, and also canonicalizes the `name` and `content` attributes.  However, the attribute values are not modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Let us take a look at the same sample page as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Tagged links</title>\n",
      "<meta name=\"googlebot\" content=\"noindex, follow, nocache\">\n",
      "<meta name=\"robots\" content=\"noindex, nofollow\">\n",
      "<META NAME=\"GEEZERBOT\" CONTENT=\"NOINDEX, NOIMAGEINDEX\">\n",
      "</head>\n"
     ]
    }
   ],
   "source": [
    "print(page[:213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta content=\"noindex, follow, nocache\" name=\"googlebot\"/>,\n",
       " <meta content=\"noindex, nofollow\" name=\"robots\"/>,\n",
       " <meta content=\"NOINDEX, NOIMAGEINDEX\" name=\"GEEZERBOT\"/>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metas = soup.find_all('meta')\n",
    "metas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "It is easy enough to canonicalize the content to lowercase.  No space splitting or listifying is done for the meta tag because Beautiful Soup knows it is special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlebot   \t noindex, follow, nocache\n",
      "robots   \t noindex, nofollow\n",
      "GEEZERBOT   \t noindex, noimageindex\n"
     ]
    }
   ],
   "source": [
    "for meta in metas:\n",
    "    print(meta['name'], \"  \\t\", meta['content'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Your particular web scraper should probably follow the advice for the generic \"robots\" name.  If you are Google, or Yandex, or Baidu, you should follow the more specific advice directed at your robot.  \n",
    "\n",
    "Some of the rest takes modest interpretation.  Indexing and caching (including separate image indexing) are obviously things general search engines do.  Your particular web crawler may or may not index pages (either the one at hand or ones followed.  There are definitely gray areas about what does or does not amount to indexing though (e.g. is a list of \"pages where the name of my company appears\" an index?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Parsing robots.txt\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "For the most part, requests from site creators to consumers about automated use of their sites lives in the special file `robots.txt`.  A single file at the root of a given domain describes what access patterns are permitted and impermissible for robots (i.e. web scrapers, spiders, automated access).\n",
    "\n",
    "This file consists of multiple sections, each pertaining to one or more patterns of access.  These instructions can describe what is permitted at all, or also how fast a robot may access a site.  Let us look at a simple hypothetical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "```\n",
    "User-agent: nicebot\n",
    "User-agent: fancybot\n",
    "Crawl-delay: 2\n",
    "# Can index archive, but not MOST 'transient' content\n",
    "Allow: /archive/\n",
    "Disallow: /transient/\n",
    "Allow: /transient/special\n",
    "```\n",
    "```\n",
    "User-agent: *\n",
    "Crawl-delay: 10\n",
    "# Other bots cannot index archive, nor query URLs\n",
    "Disallow: /archive/\n",
    "Disallow: /*?*\n",
    "Disallow: *.php$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "The pattern of what is permitted and what is not, and what might override what, can be moderately complicated to interpret.  The best way to make sure you are drawing the right conclusion, is to use `urllib.robotparser` from the standard library.  Let us start with a simple example (University of California might change this file later, but as it appears now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /directory/\n",
      "Crawl-delay: 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import robotparser\n",
    "import requests\n",
    "berkeley = 'https://www.berkeley.edu/robots.txt'\n",
    "print(requests.get(berkeley).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Let us check a few permissions for accessing Berkeley resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl delay for specific agent: 120\n",
      "Generic crawl delay for anyone: 120 \n",
      "\n",
      "I may crawl the map:   True\n",
      "I may crawl directory: False\n"
     ]
    }
   ],
   "source": [
    "parser = robotparser.RobotFileParser()\n",
    "parser.set_url(berkeley)\n",
    "parser.read()\n",
    "print(\"Crawl delay for specific agent:\", parser.crawl_delay('bad_robot'))\n",
    "print(\"Generic crawl delay for anyone:\", parser.crawl_delay('*'), \"\\n\")\n",
    "\n",
    "get_map = parser.can_fetch(\"MyRobot\", \"https://www.berkeley.edu/map/\")\n",
    "get_dir = parser.can_fetch(\"MyRobot\", \"https://www.berkeley.edu/directory/\")\n",
    "print(\"I may crawl the map:  \", get_map)\n",
    "print(\"I may crawl directory:\", get_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Berkeley was very generic in their rules. Many sites are more specific.  Let us look at Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /etext\n",
      "Disallow: /dirs/etext\n",
      "Disallow: /dirs/1\n",
      "Disallow: /dirs/2\n",
      "Disallow: /dirs/3\n",
      "Disallow: /dirs/4\n",
      "Disallow: /dirs/5\n",
      "Disallow: /dirs/6\n",
      "Disallow: /dirs/7\n",
      "Disallow: /dirs/8\n",
      "Disallow: /dirs/9\n",
      "Disallow: /catalog/world/\n",
      "Disallow: /ebooks/search\n",
      "Disallow: /ebooks/send/\n",
      "Disallow:  ...\n"
     ]
    }
   ],
   "source": [
    "pg = \"https://www.gutenberg.org/robots.txt\"\n",
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url(pg)\n",
    "rp.read()\n",
    "print(str(rp.default_entry)[:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "What things are permitted and which are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.can_fetch(\"AdsBot-Google\", \"https://www.gutenberg.org/help/faq.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.can_fetch(\"MyRobot\", \"https://www.gutenberg.org/help/faq.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.can_fetch(\"MyRobot\", \"https://www.gutenberg.org/ratelimiter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Good manners (Summary)\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "The standard for being polite to websites is fairly simple.  Create a `RobotFileParser` before you access additional resources in a particular domain.  Call `.crawl_delay(my_robot_name)` once at the start of crawling, and add delays in obtaining resources matching the request the website makes.  The parser will figure out which rule your robot name belongs to.  \n",
    "\n",
    "Then for each URL you are considering accessing, ask the `.can_fetch(my_robot_name, url)` question before actually retrieving it.  Making that call does not utilize any network connection after the initial parser is created, it just analyzes the combined rules which will not take long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img></div>\n",
    "\n",
    "Admittedly, it is not quite as simple if you look at `<meta name=\"robots\" ...>` directives and `<a rel=\"...\">` attributes.  But building all these together is not especially difficult.\n",
    "\n",
    "When you are considering adding a URL to the list of those you will crawl, check whether you obtained it via a link tag with a prohibited relation, usually \"nofollow\".  If so, probably do not add it.  When you retrieve a new page that is otherwise permitted by `robots.txt`, check the `<meta>` tag to decide whether you will obtain new links from its contents.\n",
    "\n",
    "Performing all of these checks is only a few lines of code, mostly the ones shown in this lesson.  Generalizing them to whatever web scraping library you might be using is not difficult."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
