{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing\n",
    "\n",
    "As an alternative to using Python-level threading, where concurrency is managed by the Python interpreter, is using processes managed by the operating system.  There are advantages and disadvantages to processes vs threads.  Multiprocessing uses the `multiprocessing` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Python there is a third approach as well, called *coroutines* that are managed by the standard library module `asyncio` or by one of several third party coordinating modules.  We look at those in a separate INE course; but the key idea is that they require explicitly writing \"control release\" code rather than allowing the Python interpreter or the operating system to preempt execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of multiprocessing is that it enables just as much actual parallelism as your operating system and hardware support.  If you have multiple cores, a different Python interpreter can run on each core.  Even if you have more processes than cores, the operating system will handle preemption and time-slicing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The disadvantage of multiprocessing follows directly from the advantage.  Since each process runs a different interpreter, **nothing** is shared between them after process creation.  The only way data can be communicated is with explicit *interprocess communication* (IPC) or shared memory.  IPC means `Queue`s or `Pipe`s.  Shared memory can often be faster than IPC, but are more specialize and more difficult to work with.  These include `Value`s and `Array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Word about Pipes\n",
    "\n",
    "The main interface of `multiprocessing` is very close to the interface to `threading`.  Let us import some capabilities we work with in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from time import time, sleep\n",
    "from pprint import pprint\n",
    "from multiprocessing import Process, Queue, Pipe, Pool\n",
    "from queue import Empty\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the bulk of this lesson, we will use `Queue`s to communicate.  As we saw with `threading`, this is a structure that allows concurrent access, but with a very simplified `.put()` / `.get()` interface, not random access or other sophisticated access patterns.  Note that `multiprocessing.Queue` is a completely distinct object from `queue.Queue` and the two cannot be interchanged, despite sharing most interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes you do not want a single shared data structure, but rather a private communication between two particular processes.  When that is the case, you should use a `Pipe`.  `.send()` and `.recv()` deal with arbitrary pickled objects, while `.send_bytes()` and `.recv_bytes()` (or `.recv_bytes_into()`) avoid the pickling overhead, but are limited to raw bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am process_one; process id is 3008.\n",
      "I am process_two (3011); waiting...\n",
      "Happy someone said 'Hello from process_one' to me.\n",
      "Exiting 3008\n",
      "Exiting 3011\n"
     ]
    }
   ],
   "source": [
    "sender, recipient = Pipe()\n",
    "\n",
    "def process_one(pipe):\n",
    "    print(f\"I am process_one; process id is {os.getpid()}.\\n\", flush=True, end='')\n",
    "    pipe.send(\"Hello from process_one\")\n",
    "    sleep(0.01)\n",
    "    print(f\"Exiting {os.getpid()}\\n\", flush=True, end='')\n",
    "    \n",
    "def process_two(pipe):\n",
    "    print(f\"I am process_two ({os.getpid()}); waiting...\\n\", flush=True, end='')\n",
    "    greeting = pipe.recv()\n",
    "    print(f\"Happy someone said '{greeting}' to me.\\n\", flush=True, end='')\n",
    "    print(f\"Exiting {os.getpid()}\\n\", flush=True, end='')\n",
    "\n",
    "processes = [\n",
    "    Process(target=process_one, args=(sender,)),\n",
    "    Process(target=process_two, args=(recipient,))\n",
    "]\n",
    "for p in processes: \n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Producer/Consumer Parallelism\n",
    "\n",
    "For the bulk of this lesson, let's revisit the Mandelbrot set generation from the last lesson.  It was moderately slow when done sequentially, and throwing threads at the *embarrasingly parallel* CPU-bound problem made it *slightly* worse.  Remember our `mandelbrot()` function that takes a complex coordinate, and returns the \"orbit\" at which the point \"escapes* in the iterative calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mandelbrot(z0: complex, orbits: int = 255) -> int:\n",
    "    z = z0\n",
    "    for n in range(orbits):\n",
    "        if abs(z) > 2.0:\n",
    "            return n\n",
    "        z = z * z + z0\n",
    "    return orbits\n",
    "\n",
    "# How many iterations for sample point to \"escape\"?\n",
    "mandelbrot(0.0965-0.638j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to recast this problem in a producer/consumer pattern, which you saw in the first lesson on `concurrent.futures`, we can break down the pieces.  First, we need a function that describes the collection of complex points to work with, and feeds them into a `multiprocessing.Queue` named `Q_todo`.  Batching the data is vastly more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def produce_points(Q_todo: Queue, pixdim: int, \n",
    "                   escape=255, x=0.1015, y=-0.633, size=0.01):\n",
    "    # Generate the complex coords and queue them\n",
    "    batch, npoints = [], pixdim**2\n",
    "    for row, col in product(range(pixdim), range(pixdim)):\n",
    "        real = x - (size/2) + (size * col/pixdim)\n",
    "        imag = y - (size/2) + (size * row/pixdim)\n",
    "        data = ((row, col), complex(real, imag))\n",
    "        batch.append(data)\n",
    "        if len(batch) % 1000 == 0:\n",
    "            Q_todo.put_nowait(batch)\n",
    "            batch = []\n",
    "\n",
    "    print(f\"Queued {npoints:,} points in process {os.getpid()}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The next thing we need is a function that will read complex `z0` points off the TODO queue, call the `mandlebrot()` function, and put the answer onto the RESULTS queue.  We batch these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def process_points(Q_todo: Queue, Q_result: Queue, escape: int = 255) -> int:\n",
    "    # Each item in Q_todo looks like: `((row, col), coord)`\n",
    "    # After processing, push to Q_result like: `((row, col), orbit)`\n",
    "    start = time()\n",
    "    try:\n",
    "        i = 0\n",
    "        while batch := Q_todo.get(timeout=1):\n",
    "            done = []\n",
    "            for point in batch:\n",
    "                (row, col), z0 = point\n",
    "                done.append( ((row, col), mandelbrot(z0, escape)) )\n",
    "            Q_result.put(done)\n",
    "            i += len(batch)\n",
    "    except Empty as err:\n",
    "        duration = time() - start\n",
    "        print(f\"Processed {i:,} points in process {os.getpid()} \"\n",
    "              f\"({duration:0.2f} seconds)\\n\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final step is not part of the computation per-se, but it allows us to utilize it.  We can take data off the RESULTS queue, and put it into the array `canvas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fill_canvas(Q_result, canvas):\n",
    "    try:\n",
    "        i = 0\n",
    "        while batch := Q_result.get(timeout=1):\n",
    "            for result in batch:\n",
    "                (row, col), orbit = result\n",
    "                canvas[row, col] = orbit\n",
    "            i += len(batch)\n",
    "    except Empty as err:\n",
    "        print(f\"Filled canvas with {i:,} points\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us run these functions purely sequentially.  Starting by producing the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queued 2,560,000 points in process 2983\n",
      "CPU times: user 2.19 s, sys: 188 ms, total: 2.38 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q_todo, Q_result = Queue(), Queue()\n",
    "pixdim = 1600\n",
    "canvas = np.zeros(shape=(pixdim, pixdim), dtype=np.uint8)\n",
    "\n",
    "produce_points(Q_todo, pixdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_todo.qsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we process those points that have been placed on the TODO queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_points(Q_todo, Q_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pull the array of escape orbits into the local array `canvas` from `Q_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fill_canvas(Q_result, canvas)\n",
    "canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the canvas/array\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.imshow(canvas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trying Processes\n",
    "\n",
    "With the nicely modularized functions, we might try to run the actual computations in parallel across multiple processes.  First, let us check the status of the queues, and what process we are running inside now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"   TODO queue items:\", Q_todo.qsize())\n",
    "print(\"RESULTS queue items:\", Q_result.qsize())\n",
    "print(\" Current process ID:\", os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the actual work.  We launch separate OS processes for `produce_points()` and for multiple copies of `process_points()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processes = []\n",
    "\n",
    "producer = Process(target=produce_points, args=(Q_todo, pixdim))\n",
    "producer.start()\n",
    "processes.append(producer)\n",
    "sleep(0.5)  # Allow producer to start on filling queue\n",
    "\n",
    "for _ in range(6):\n",
    "    p = Process(target=process_points, args=(Q_todo, Q_result))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "while Q_todo.qsize() > 0:\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to empty the queues to fill the canvas.  This *could* be done within a process, but we would need to orchestrate some short of shared memory to actually use it within this notebook.  Just reading queue locally is easier in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fill_canvas(Q_result, canvas)\n",
    "print(f\"Queue sizes: {Q_todo.qsize()}, {Q_result.qsize()}\")\n",
    "canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Process Pools\n",
    "\n",
    "The `multiprocessing` module contains a very useful technique called a `Pool` that lets you describe operations to be performed in processes with a higher level abstraction.  This creates several processes within a context manager, and a few operations will simply take the \"next available\" process in which to perform that.\n",
    "\n",
    "To demonstrate, let's create a small collection of points that were in our Mandelbrot rendering.  Processing 15 points takes a negligible time, but we could use this organization for larger collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 23, 47, 43, 18, 39, 59, 69, 60, 23, 47, 43, 18, 39, 57]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = [0.102125-0.63780625j, 0.098375-0.6378j, 0.104625-0.6378j,\n",
    "          0.100875-0.63779375j, 0.097125-0.6377875j, 0.103375-0.6377875j, \n",
    "          0.099625-0.63778125j, 0.105875-0.63778125j, 0.102125-0.637775j,\n",
    "          0.098375-0.63776875j, 0.104625-0.63776875j, 0.100875-0.6377625j,\n",
    "          0.097125-0.63775625j, 0.103375-0.63775625j, 0.099625-0.63775j]\n",
    "\n",
    "[mandelbrot(c) for c in points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using a `Pool`, we can define work to be performed, but the various processes will complete that work asynchronously.  There exists an `.apply()` method along with `.apply_async()`, but it is fairly worthless since it blocks for the process to become ready, defeating the purpose of multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<multiprocessing.pool.ApplyResult object at 0x79d525d40d60>,\n",
      " <multiprocessing.pool.ApplyResult object at 0x79d525d40eb0>,\n",
      " <multiprocessing.pool.ApplyResult object at 0x79d525d408b0>]\n",
      "Ready? [False, False, False]\n",
      "Sleeping...\n",
      "Ready? [True, True, True]\n",
      "Results: [49, 23, 47, 43, 18, 39, 59, 69, 60, 23, 47, 43, 18, 39, 57]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=8) as pool:\n",
    "    results = [pool.apply_async(mandelbrot, (c,)) for c in points]\n",
    "    pprint(results[:3])\n",
    "    print(\"Ready?\", [r.ready() for r in results[:3]])\n",
    "    print(\"Sleeping...\"); \n",
    "    sleep(0.1)\n",
    "    print(\"Ready?\", [r.ready() for r in results[:3]])\n",
    "    print(\"Results:\", [r.get() for r in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We might simplify this pattern of using a `Pool` even more by `.map()`ing data values to a single function.  Like `.apply()`, `.map()` blocks—but in this case it is not pointless since it will allocate the first N data items to the N processes launched before blocking.  As each result becomes available, that process is used to process the next data value.  You can also use `.map_async()` instead. Some slightly more exotic—but useful—variations like `.starmap()`, `.imap()`, `.imap_unordered()`, and `starmap_async()` are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [49, 23, 47, 43, 18, 39, 59, 69, 60, 23, 47, 43, 18, 39, 57]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=10) as pool:\n",
    "    results = pool.map(mandelbrot, points)\n",
    "    print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: <multiprocessing.pool.MapResult object at 0x79d525d683d0>\n",
      "Values (blocking): [49, 23, 47, 43, 18, 39, 59, 69, 60, 23, 47, 43, 18, 39, 57]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=10) as pool:\n",
    "    results = pool.map_async(mandelbrot, points, chunksize=3)\n",
    "    print(\"Results:\", results)\n",
    "    print(\"Values (blocking):\", results.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson we have only scratched the surface of `multiprocessing` module.  This lesson has hinted at or briefly mentioned most of what is in that module, but much of it was, of necessity, cursory. \n",
    "\n",
    "Working with multiple processes has different challenges than working with threads.  We mostly avoid the issues of race conditions and deadlocks, but not entirely since `multiprocessing.Lock` is also available, and you do need to use it sometimes.  But comparatively, queues are much slower in mutliprocessing than in multithreading, and process creation is much slower than thread creation.\n",
    "\n",
    "The times when you really want multiprocessing are when you have substantial CPU-bound work that is parallelizable.  For the embarrassingly parallel cases, a producer/consumer model is a good choice.  For for complex structures, use of dedicated processes and pipes between them is often better."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
