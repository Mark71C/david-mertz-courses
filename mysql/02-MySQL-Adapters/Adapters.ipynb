{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h1 style=\"color: white; position: absolute; top:27%; left:10%;\">\n",
    "    MySQL and MariaDB for Python Developers\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:55%; left:10%;\">\n",
    "    David Mertz, Ph.D.\n",
    "</h3>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:62%; left:10%;\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL and MariaDB adapters for Python\n",
    "\n",
    "Most Python developers who access MySQL databases use the `mysql.connector` driver/adapter that is provided by Oracle and the MySQL project.  It is well tested, reliable, and conforms well to the DB-API 2.0 standard.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are several other notable options for adaptors that you may want to consider for your particular use case.\n",
    "\n",
    "* `PyMySQL` is a pure-Python implementation that complies with DB-API 2.0.  In the base version, it has no external dependencies outside of the standard library.  However, if you wish to use the authentication methods \"sha256_password\" or \"caching_sha2_password\", you will need an extra module.  If you work in a context where pure-Python is required, this is available; some extra capabilities are also available. Both can be installed with:\n",
    "\n",
    "```bash\n",
    "pip install PyMySQL[rsa]\n",
    "```\n",
    "\n",
    "* `aiomysql` is an `asyncio`-friendly modification of `PyMySQL` that allows you to use MySQL in asynchronous programs.  General programming styles and benefits of async are discussed in another INE course.  In a sentence though, for high-performance I/O bound programs an asyncronous approach can be *vastly* faster than a thread-based or single-threaded one.  `aiomysql` is *mostly* similar to the DB-API, but because of the nature of asynchronous programming, some differences arise.\n",
    "\n",
    "* `trio_mysql` is very similar, in concept, to `aiomysql`, but it wraps `PyMySQL` for use with the `Trio` async library rather than the Python standard library `asyncio`.  There are certainly certain advantages of Trio over `asyncio`, but that is far afield of this course, and hence examples of `trio_mysql` will not be presented in this lesson.\n",
    "\n",
    "* `tormysql` is yet another asynchronous driver for MySQL.  It claims to be the highest performing driver, but I have not benchmarked it myself.  While `tormysql` can be used with `asyncio`, it focuses on another 3rd-party asynchronous library called `Tornado`.  Tornado likewise has good features, but they are outside the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-relational mapping\n",
    "\n",
    "Not covered in this course is object-relational mapping libraries that convert SQL and tuple-level interfaces with MySQL into Python method calls.  In particular, `SQLAlchemy` is very popular among many Python developers.  Personally, I find the extra layer between code and database a distraction rather than a benefit.\n",
    "\n",
    "In any event though, `SQLAlchemy` relies on the actual MySQL adapters we discuss in this lesson.  The abstractions it provides are not MySQL specific, but generic patterns for accessing RDBMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Python\n",
    "\n",
    "Let us use the `PyMySQL` adapter to load some data into our database.  Two of the enhancements in PyMySQL are more flexible cursors and providing connections and cursors as context managers.  Let us illustrate both of those in these examples.  In general, other than enhancements we might use, `PyMySQL` should be a drop-in replacement for `mysql.connector` (likely somewhat slower in pure-Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMySQL version | 0.9.3\n",
      "API level       | 2.0\n",
      "Parameter style | pyformat\n",
      "Thread safety   | 1\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "print(f\"PyMySQL version | {pymysql.__version__}\")\n",
    "print(f\"API level       | {pymysql.apilevel}\")\n",
    "print(f\"Parameter style | {pymysql.paramstyle}\")\n",
    "print(f\"Thread safety   | {pymysql.threadsafety}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need credentials and host/port setting for any of these adaptors, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `port` MUST BE an int not a string representing int for pymysql\n",
    "credentials = dict(host='localhost',\n",
    "                   user='ine_student',\n",
    "                   password='ine-password',\n",
    "                   database='ine',\n",
    "                   port=3306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_on': datetime.datetime(2021, 1, 9, 22, 19, 3), 'username': 'Alice'},\n",
      " {'created_on': datetime.datetime(2021, 1, 9, 22, 19, 3), 'username': 'Bob'},\n",
      " {'created_on': datetime.datetime(2021, 1, 9, 22, 19, 3), 'username': 'Carlos'},\n",
      " {'created_on': datetime.datetime(2021, 1, 9, 22, 32, 16), 'username': 'Sybil'},\n",
      " {'created_on': datetime.datetime(2021, 1, 9, 22, 32, 16), 'username': 'Trudy'},\n",
      " {'created_on': datetime.datetime(2021, 1, 9, 22, 32, 16), 'username': 'Vanna'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "# Note: context manager usage slightly different in later versions\n",
    "with pymysql.connect(charset='utf8', cursorclass=DictCursor, **credentials) as cur:\n",
    "    cur.execute(\"SELECT username, created_on FROM users;\")\n",
    "    data = cur.fetchall()\n",
    "        \n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding on schemata\n",
    "\n",
    "As some data to load into the database, let us take some information on United States zip codes published by the U.S. Census Bureau.  We have two source files available.  One tab separated file that gives explanations of column names, and another that gives information per zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS\tUnited States Postal Service State Abbreviation\n",
      "GEOID\tGeographic Identifier - fully concatenated geographic code (State FIPS and district number)\n",
      "ALAND\tLand Area (square meters) - Created for statistical purposes only\n",
      "AWATER\tWater Area (square meters) - Created for statistical purposes only\n",
      "ALAND_SQMI\tLand Area (square miles) - Created for statistical purposes only\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../data/census-zipcodes-2018.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS\tALAND\tAWATER\tALAND_SQMI\tAWATER_SQMI\tINTPTLAT\tINTPTLONG\n",
      "00601\t166659749\t799292\t64.348\t0.309\t18.180555\t-66.749961\n",
      "00602\t79307535\t4428429\t30.621\t1.71\t18.361945\t-67.175597\n",
      "00603\t81887185\t181411\t31.617\t0.07\t18.455183\t-67.119887\n",
      "00606\t109579993\t12487\t42.309\t0.005\t18.158327\t-66.932928\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../data/census-zipcodes-2018.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before putting the data into tables, we should decide on good table layouts.  The field key is relatively straightforward.  MySQL is relative strict in its permitted SQL syntax; for example, a column named `key` is not permitted (because it looks like the keyword), although other RDBMSs allow this.  Likewise, MySQL is slightly uncommon in declaring `PRIMARY KEY` as a separate pseudo-column rather than an adjective to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(**credentials)\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute('DROP TABLE IF EXISTS census_zipcode_fields;')\n",
    "    sql_census_fields = \"\"\"\n",
    "    CREATE TABLE census_zipcode_fields (\n",
    "      fieldname VARCHAR(15) NOT NULL,\n",
    "      description TEXT,\n",
    "      PRIMARY KEY (fieldname)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.execute(sql_census_fields)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set describing fields is small, and can easily be read into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/census-zipcodes-2018.fields') as fields:\n",
    "    rows = [tuple(line.strip().split('\\t')) for line in fields]\n",
    "\n",
    "sql = \"INSERT INTO census_zipcode_fields VALUES (%s, %s)\"\n",
    "\n",
    "conn = pymysql.connect(**credentials)\n",
    "with conn.cursor() as cur:\n",
    "    cur.executemany(sql, rows)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types for the main data allows us to use the data types of MySQL more versatilely.  \n",
    "\n",
    "The `DECIMAL` or `NUMERIC` types in MySQL (and other SQL systems) uses somewhat strange naming: \"precision\" means the total number of digits stored; \"scale\" means the number of those digits to the right of the decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_geography = \"\"\"\n",
    "CREATE TABLE census_zipcode_geography (\n",
    "  USPS CHAR(5),    \n",
    "  ALAND BIGINT,              -- some zips are larger than 2e9 m^2\n",
    "  AWATER BIGINT,\n",
    "  ALAND_SQMI DECIMAL(8, 3),  -- largest zips need 5 to left of decimal\n",
    "  AWATER_SQMI DECIMAL(8, 3), -- sizes with 3 digits of \"scale\"\n",
    "  INTPTLAT REAL,             -- keep fields from key, although duplicative\n",
    "  INTPTLONG REAL,\n",
    "  location POINT,             -- use geometric type for lat/lon\n",
    "  PRIMARY KEY (USPS)\n",
    ");\n",
    "\"\"\"\n",
    "conn = pymysql.connect(**credentials)\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute('DROP TABLE IF EXISTS census_zipcode_geography;')\n",
    "    cur.execute(sql_geography)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stipulate that this data is large enough we do not want to load it all at once (really it is not).  Unfortunately, PyMySQL is somewhat limited in correctly templating MySQL, and MySQL itself somewhat inflexible in accepting non-conforming SQL.\n",
    "\n",
    "Specifically, if we use the cursor templating mechanism by passing in a second argument, the constructor for `POINT` is quoted in the insertion, but MySQL will only accept the unquoted form.  We can succeed using manual Python templating.  We also need to make sure zip codes are in extra quotes so they do not get converted as integers (some have leading zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSERT into census_zipcode_geography\n",
      "VALUES ('99929', 5635963110, 637274792, 2176.058, 246.053, 56.370538, -131.693453, ST_PointFromText('POINT (56.370538 -131.693453)'));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_insert_geo = f\"\"\"\n",
    "INSERT into census_zipcode_geography\n",
    "VALUES ('%s', %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "conn = pymysql.connect(**credentials)\n",
    "with conn.cursor() as cur:\n",
    "    with open('../data/census-zipcodes-2018.tsv') as fh:\n",
    "        next(fh)   # discard header line\n",
    "        for line in fh:\n",
    "            row = line.strip().split('\\t')\n",
    "            row.append(f\"ST_PointFromText('POINT ({row[-2]} {row[-1]})')\")\n",
    "            sql = sql_insert_geo % tuple(row)\n",
    "            cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "# Show an example of the necessary SQL formatting\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see that our data is in the database, and as a preview of the POINT data type, let us make a query for the land area of those zipcodes that are close to where I live. For this short distance, the Pythagorean formula suffices; for larger distances, we should utilize Haversine distance (we return to that in a later lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(USPS='04443', ALAND_SQMI=Decimal('137.384'), AWATER_SQMI=Decimal('8.674'))\n",
      "Row(USPS='04479', ALAND_SQMI=Decimal('38.441'), AWATER_SQMI=Decimal('1.359'))\n",
      "Row(USPS='04930', ALAND_SQMI=Decimal('53.969'), AWATER_SQMI=Decimal('2.373'))\n",
      "Row(USPS='04939', ALAND_SQMI=Decimal('37.670'), AWATER_SQMI=Decimal('0.269'))\n"
     ]
    }
   ],
   "source": [
    "sql_near = \"\"\" -- My lat/lon: approx 45.1, -69.3\n",
    "SELECT USPS, ALAND_SQMI, AWATER_SQMI\n",
    "FROM census_zipcode_geography \n",
    "-- search within approximately 0.15 degrees \n",
    "-- but lat/lon are different sizes away from equator\n",
    "WHERE SQRT ( POWER(ST_X(location)-45.1, 2) + \n",
    "             POWER(ST_Y(location)+69.3, 2) ) < 0.15;\n",
    "\"\"\"\n",
    "with pymysql.connect(**credentials) as cur:\n",
    "    cur.execute(sql_near)\n",
    "    results = cur.fetchall()\n",
    "    Row = namedtuple(\"Row\", [c[0] for c in cur.description])\n",
    "\n",
    "for row in results:\n",
    "    print(Row(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous access\n",
    "\n",
    "On modern computers, I/O is by far the slowest component.  Thread switches, let alone process switches, are relatively expensive.  Simply checking whether a given I/O operation is ready to provide more data is one or two orders of magnitude cheaper, and has zero memory cost compared to allocating a thread.  \n",
    "\n",
    "Using `aiomysql` (or `trio_mysql` or `tormysql`) allows your program to perform other work while waiting for the results to arrive from a query.  However, doing so *does* require becoming familiar with the `await` and `async` keywords, and generally shifting your thinking towards the styles of programming required by `asyncio` in the standard library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple examples in this lesson will not come remotely close to those where any of this matters.  But for much larger datasets, and for multi-tenancy of RDBMS access, the differences can be huge.\n",
    "\n",
    "We first will import the `asyncio` scaffolding and the `aiomysql` module.  Because `asyncio` does not claim to follow the DB-API, the module attributes like `.apilevel`, and `.paramstyle` do not exist.  However, *most* of the DB-API is still consistent; e.g. `.connect()`, `.cursor()`, `.execute()`, `.fetchall()` still have their familiar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from asyncio import get_event_loop, gather, as_completed\n",
    "import aiomysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this code is running inside a Jupyter notebook which already has its own `asyncio` event loop, we need to use a third-party module called `nest_asyncio` to path the event loop and run async code in cells.  Outside of environments (Jupyter, web servers, GUI applications) that might create their own event loops, this is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to check the zip codes near certain latitude/longitude locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"latitude longitude distance\")\n",
    "locs = [Location(40.0, -105.3, 0.15), Location(45.1, -69.3, 0.15), \n",
    "        Location(34.9, -82.4, 0.15), Location(42.6, -72.5, 0.15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `port` MUST BE an int not a string representing int for pymysql\n",
    "# Named of database is `db` rather than `database` parameter for aiomysql\n",
    "credentials = dict(host='localhost',\n",
    "                   user='ine_student',\n",
    "                   password='ine-password',\n",
    "                   db='ine',\n",
    "                   port=3306)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an asyncronous adapter, we need to wrap our operation in a special coroutine function that is defined with `async def` rather than plain `def`.  Each of the steps has an extra `await` keyword to indicate that the event loop is free to do other work between each such line.  The logic, however, is very much the same as we have seen with other adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def near_location(loc):\n",
    "    conn = await aiomysql.connect(**credentials)\n",
    "    cur = await conn.cursor()\n",
    "    await cur.execute(sql_near % loc)\n",
    "    results = await cur.fetchall()\n",
    "    return (loc, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot simply run this function, but need instead to tell the event loop to manage it.  In fact, let us let the event loop handle several such coroutines, each for a different reference location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_near = \"\"\" -- templatized where near and distance\n",
    "SELECT USPS, ALAND_SQMI, AWATER_SQMI\n",
    "FROM census_zipcode_geography \n",
    "-- search within approximately 0.15 degrees \n",
    "-- but lat/lon are different sizes away from equator\n",
    "WHERE SQRT ( POWER(ST_X(location)-%f, 2) + \n",
    "             POWER(ST_Y(location)-%f, 2) ) < %f;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(latitude=40.0, longitude=-105.3, distance=0.15) ... 10 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('80025', Decimal('11.722'), Decimal('0.004'))\n",
      "('80027', Decimal('19.462'), Decimal('0.196'))\n",
      "\n",
      "Location(latitude=45.1, longitude=-69.3, distance=0.15) ... 4 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('04443', Decimal('137.384'), Decimal('8.674'))\n",
      "('04479', Decimal('38.441'), Decimal('1.359'))\n",
      "\n",
      "Location(latitude=34.9, longitude=-82.4, distance=0.15) ... 12 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('29601', Decimal('4.280'), Decimal('0.024'))\n",
      "('29605', Decimal('25.579'), Decimal('0.175'))\n",
      "\n",
      "Location(latitude=42.6, longitude=-72.5, distance=0.15) ... 13 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('01054', Decimal('22.790'), Decimal('0.164'))\n",
      "('01301', Decimal('25.494'), Decimal('0.541'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aws = [near_location(loc) for loc in locs]\n",
    "\n",
    "loop = get_event_loop()\n",
    "for ret in loop.run_until_complete(gather(*aws)):\n",
    "    loc, results = ret\n",
    "    print(loc, \"...\", len(results), \"tuples\\n\", \"-\"*70)\n",
    "    for tup in results[:2]:\n",
    "        print(tup)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has two limitations.  \n",
    "\n",
    "* Each time a new coroutine is created, it makes a new connection.  A more efficient approach is to create a *connection pool* and share connections as they are requested (but not close them implicitly at function end).\n",
    "* We wait for all the results from the various coroutines to be complete in `loop.run_until_complete()`.  If the 4th query is ready early, we cannot process it while waiting for the 1st query to complete.\n",
    "\n",
    "As a secondary concern, by doing a `.fetchall()` on the results, we cannot not process each result tuple immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def one_location(pool, loc):\n",
    "    async with pool.acquire() as conn:\n",
    "        async with conn.cursor() as cur:\n",
    "            await cur.execute(sql_near % loc)\n",
    "            results = []\n",
    "            async for row in cur:\n",
    "                # might process each tuple as soon as received\n",
    "                results.append(row)\n",
    "    return (loc, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def near_locations(locs):\n",
    "    async with aiomysql.create_pool(**credentials, maxsize=10) as pool:\n",
    "        queries = [one_location(pool, loc) for loc in locs]\n",
    "        for future in as_completed(queries):\n",
    "            loc, results = await future\n",
    "            print(loc, \"...\", len(results), \"tuples\\n\", \"-\"*70)\n",
    "            for tup in results[:2]:\n",
    "                print(tup)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(latitude=45.1, longitude=-69.3, distance=0.15) ... 4 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('04443', Decimal('137.384'), Decimal('8.674'))\n",
      "('04479', Decimal('38.441'), Decimal('1.359'))\n",
      "\n",
      "Location(latitude=42.6, longitude=-72.5, distance=0.15) ... 13 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('01054', Decimal('22.790'), Decimal('0.164'))\n",
      "('01301', Decimal('25.494'), Decimal('0.541'))\n",
      "\n",
      "Location(latitude=40.0, longitude=-105.3, distance=0.15) ... 10 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('80025', Decimal('11.722'), Decimal('0.004'))\n",
      "('80027', Decimal('19.462'), Decimal('0.196'))\n",
      "\n",
      "Location(latitude=34.9, longitude=-82.4, distance=0.15) ... 12 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('29601', Decimal('4.280'), Decimal('0.024'))\n",
      "('29605', Decimal('25.579'), Decimal('0.175'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()  \n",
    "loop.run_until_complete(near_locations(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For users who want to venture beyond the standard `mysql.connector` adapter, several other good options are available.  For heavy workloads, using one of the asynchronous adapters can be a big win. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
